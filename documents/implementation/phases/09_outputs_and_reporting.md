# Phase 09 – Outputs & Reporting

## Objective
Deliver stakeholder-ready assets that translate cleaned and forecasted data into actionable insights for executives, sales planning, marketing, and BI governance. This phase codifies the final artifacts, their refresh cadence, and ownership.

## Primary Deliverables
- `analytics.forecast_overall` – Quarterly-level forecasts with total quantity/revenue per cohort (`forecast_run_id`, `cohort`, `model_name`, `model_version`, `sku`, `channel`, `sale_month`, `forecast_date`, `forecast_quantity`, `forecast_revenue`, `forecast_revenue_lower`, `forecast_revenue_upper`, `actual_quantity`, `actual_revenue`, `product_name`, `product_type`, `product_subcategory`, `created_at`). Supplies the executive dashboard and top-line communications.
- `analytics.forecast_by_customer` – Customer-level projections with confidence metrics for sales/account planning (extends core columns above with customer identifiers and tier tags).
- `analytics.forecast_metrics` – Run-level QA table (MAPE, RMSE, coverage, timestamp, data slice) monitored by BI quality and SRE teams.
- `analytics.c2_adoption_scorecard` – C2 exploratory scorecard (`snapshot_month`, `service_family`, `plan_variant`, `region`, `customer_tier`, `sku`, `active_subscriptions`, `new_subscriptions`, `mrr_usd`, `total_quantity`, `avg_seats`) used in Metabase narratives.
- `analytics.activation_storylines` – Commercial activation narrative feed (`snapshot_month`, `region`, `segment`, `actual_revenue`, `forecast_revenue`, `variance_pct`, `inventory_weeks_cover`, `market_index_score`, `activation_score`, `recommended_action`, `confidence_note`) informing campaign storytelling.
- `documents/reports/forecast_insights_report.md` (or HTML export) – Narrative summary of trends, anomalies, and business callouts consumed by marketing/strategy stakeholders.
- Metabase dashboards:
  - `metabase_forecast_dashboard` – Actual vs forecast by region, customer, product family.
  - `metabase_forecast_health` – Monitoring panel highlighting forecast metrics and ingestion freshness.
- Airflow task lineage & logs – Embedded links for auditors to confirm the originating DAG run and Airflow-generated metadata.

## Inputs
- Cleaned sales parquet (`data/processed/synosales_cleaned.parquet`) and `stg_synology_sales` warehouse table.
- Forecast artifacts generated by Phase 04 (CSV/JSON) and feature marts from Phase 03.
- Metadata/QA output (retry counts, dbt test results) from Phase 06.

## Workflow Checklist
1. **Ingest Forecast Tables** – Load CSV outputs from `data/processed/forecasts/` into warehouse tables (`analytics.forecast_*`) using dbt seeds or Python loaders with schema enforcement.
2. **Publish Dashboard Models** – Ensure dbt models (or views) expose the latest forecasts alongside historical actuals for Metabase; document joins and filters in schema.yml.
3. **Generate Insights Report** – Run templated notebook/script that translates forecast results into markdown/HTML, embedding key visuals and KPI deltas; archive versions per release.
4. **Quality Review** – Validate forecast metrics against thresholds (MAPE target per KPI baseline) and document outcomes in `forecast_metrics`; surface blockers via Airflow SLAs/alerts.
5. **Stakeholder Communication** – Post dashboards/reports to agreed channels (Metabase collections, Slack, email digest) with summary bullets and required actions.

## Change Management
- Update this document when new deliverables are introduced (e.g., customer-segment dashboards, additional QA tables) or when ownership changes.
- Track schema/version changes in dbt docs and data dictionary; archive superseded forecast reports for traceability.
- Coordinate with Deployment/Ops (Phase 08) to ensure refresh schedules align with forecast consumption windows.

## Ownership & Contacts
- **BI Engineering** – Maintains dbt models, warehouse tables, Metabase datasets.
- **Data Science / Forecasting** – Owns forecast generation, metrics validation, and insight narratives.
- **Marketing & Sales Strategy** – Primary consumers; provide feedback on report usefulness and required enhancements.

## Success Criteria
- All deliverables refresh automatically post `syno_forecast_*` DAG completion.
- Stakeholders have a single source of truth for forecast numbers within hours of data availability.
- QA metrics (forecast accuracy, data freshness) are visible and monitored via Metabase or alerting.
- Documentation (this phase, data dictionary, insights reports) stays in lock-step with pipeline evolution.
